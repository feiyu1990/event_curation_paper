\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{color}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{mathtools}
%\usepackage{authblk}


\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
           {1.25ex \@plus1ex \@minus.2ex}%
           {-1em}%
           {\normalfont\normalsize\bfseries}}
            
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                {-2ex\@plus -1ex \@minus -.2ex}%
                {0.1ex \@plus .2ex}%
                {\normalfont\normalsize\bfseries}}
                
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{1798} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
%\title{Creating Album Highlights Automatically}
\title{Event-specific Image Importance}

%\author[1]{Author A\thanks{A.A@university.edu}}
%\author[1]{Author B\thanks{B.B@university.edu}}
%\author[1]{Author C\thanks{C.C@university.edu}}
%\author[2]{Author D\thanks{D.D@university.edu}}
%\author[2]{Author E\thanks{E.E@university.edu}}
%\affil[1]{UCSD, \LaTeX\ University}
%\affil[2]{Department of Mechanical Engineering, \LaTeX\ University}


\author{Yufei Wang\\
UCSD\\
San Diego\\
{\tt\small yuw176@ucsd.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
When creating a photo album of an event, people typically select a few important images to keep or share. There is some consistency in the process of choosing the important images, and discarding the unimportant ones. Modeling this selection process will assist automatic photo selection and album summarization.
In this paper, we show that the selection of important images is consistent among different viewers, and that this selection process is related to the event type of the album. We introduce the concept of event-specific image importance. We collected a new event album dataset with human annotation of the relative image importance. We also propose a Convolutional Neural Network (CNN) based method to predict the image importance score of a given event album, using a novel rank loss function and a progressive training scheme. Results demonstrate that our method significantly outperforms various different baseline methods.

 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
With the proliferation of cameras (in cell phones and other portable cameras), taking photographs is practically effortless, and happens frequently in everyday life. When attending an event, for instance, a Thanksgiving holiday, participants often take many photos recording every interesting moment during the event. This leads to an oversized album at the end of the event. When we need to simplify the album before saving to a device, or if we want to make a photo collage or a photo book to share our important moment %(for example, our wedding ceremony) 
with others, we have to go through the tedious and time-consuming work of selecting important images from a large album. Therefore, it is desirable to perform this task automatically.

% Moreover, it's often the case that we take photos that are not interesting to keep anymore when we look back at the album, such as photos of food. 

%When we save the album to device, the tedious and time-consuming work of manually cleaning up the album often makes us leave the large album disordered and redundant as it is, and this often leads to our reluctancy to look back at this oversized album in the future.

%On the other hand, if we want to make a photo collage or a photo book to share our important moment (for example, our wedding ceremony) with others, we have to go through the tedious work of selecting important images from the large album. This is even true with photos taken by professional photographers, because most of the time the album is much larger and redundant than what we want to keep.

%Both the need of keeping and sharing photo albums lead to the need to select an important subset of images from a larger album automatically, and it's desirable to keep the user input as little as possible.

Automatic photo selection or album summarization has been studied by some researchers\cite{nips_summarize, vacation, sum_pinaki, photo_selection1, gaze}. They aim at personal event albums, and visual content information as well as diversity and coverage of is often considered jointly to obtain a summarization. 
However, these works ignored the role of the event type in the selection process. Intuitively, the event type of the album is an important criterion when we select important images. For example, if we need to select important photos from a vacation to Hawaii, the photo of the volcano on the Big Island is definitely important to keep, whereas if the album is the wedding ceremony, beautiful scenes are only background and are not likely to be more important than the shot of the bride and groom. 

Walber \etal showed with a user study that single image properties such as attractiveness and image interestingness are important criteria for humans to do photo selection, while diversity and coverage of the selection is less important \cite{gaze}. 

In this paper, we introduce the concept of event-specific image importance. It is different from general image interestingness, in that it is contextual, and is based on the album the image is in. We focus on the event-specific importance score of a single image, and do not consider summarization problems where diversity and coverage are also important: Image importance prediction is the most challenging and crucial part of the event curation/album summarization process; Moreover, the importance score can be directly applied to to any album summarization algorithm. We collect an event-specific image importance dataset from human annotators, and we show that the event-specific importance is subjective yet predictable. Finally, we provide a method for predicting event-specific image importance using Convolutional Neural Network (CNN). We propose a new loss function and learning procedure, and our CNN method greatly outperforms different baselines.

\section{Related Work}

\paragraph{Image properties.}
\textcolor{red}{Importance of an image is a complex image property, and is related to many other image properties. Many image properties can be viewed as cues when selecting important images, such as memorability \cite{Isola2011, NIPS2011_4451}, specificity \cite{specificity}, popularity \cite{www14_khosla}, aesthetics and interestingness \cite{interesting, interesting2}. Those image properties are correlated to image contents, such as high level features: object and scene categories \cite{Isola2011, NIPS2011_4451, specificity, www14_khosla, interesting}, and low level features: texture, edge distribution, etc.\cite{interesting2, www14_khosla}. In this work, rather than the general image properties mentioned above, we study the event-specific image importance, which is human preferences related to images within the context of an album, where the album is of a known event type.}
%Without the context of an album, general high level properties of one image are studied by several works. \cite{Isola2011, NIPS2011_4451} show that memorability is correlated to object and scene category, and enclosed space and human presence increase memorability of an image. Image popularity also correlates with object categories \cite{www14_khosla}. \cite{specificity} shows image specificity is correlated to image content, and correlated to memorability as well. \cite{interesting} uses high level attributes to predict aesthetics.  \cite{interesting2} shows interestingness is predictable from several cues. Rather than general image properties, we study human preference to images with a context of an album and with the information of event type of the album. 

\paragraph{Convolutional Neural Networks(CNNs).}
The development of methods for training deep CNNs has led to rapid progress in many computer vision tasks in recent years.  Substantial improvements have been made in basic computer vision problems such as image classification \cite{imagenet, googlenet}, object detection \cite{rcnn, devil} and scene recognition \cite{places, scene}. Now, there is a greater focus on learning higher-level image properties. One example closely related to our project is Xiong \etal's work on event recognition from static images \cite{event_recognition}. In this work, the network is divided into different channels, creating human and object maps that are then fused with the original images to jointly train a deep architecture that predicts the event type from a single image. Our model also uses deep representations to capture event features, but our  focus is on event curation rather than event recognition. This requires choosing the most important images for the event in question.

\paragraph{Album summarization and photo selection.}
\textcolor{red}{The work that is most related to our project is work on summarization and selection from an album or several albums.}

\textcolor{red}{There are some works focusing on summarization of public photo/video collections from different sources for well-known social events or sceneries\cite{social_summarize, scene_summarize}. Tschiatschek \etal focuses on single album summarization \cite{nips_summarize}. They consider coverage and diversity of the albums and do not take image importance score as a cue. For the same problem of summarization of personal albums, image quality as well as coverage and diversity is used in \cite{vacation, sum_pinaki}. Single image quality is only one of the factors, and image quality is captured by general aesthetics or interestingness which does not take event-specific information into account. Sinha \etal aims at summarization of personal photo collections taken in a long time span, and it takes the event type as one photo descriptor to calculate diversity and coverage of the photo subset \cite{sum_pinaki}. Instead, we focus on an album taken from a single event, and we use event type information when predicting image importance.}

\textcolor{red}{For photo selection problem, Yeh \etal proposed a ranking system for photographs based on a set of aesthetic rules and personal preferences \cite{perosnalized_che} . This work is for photography, not personal albums.  Walber \etal uses gaze information from user's photo viewing process to assist the automatic photo selection algorithm, and this work needs additional gaze information from the user, which is hard to acquire \cite{gaze}.}

\textcolor{red}{The work by Ceroni \etal \cite{photo_selection1} is probably most relevant to our work. It focuses on selection of important photos from a single event album, and different factors are considered: image quality, presences of faces, concept features, and collection based features such as album size. However, each album used for training and testing in this work is collected from a single participant and the important subset is picked by the same person: it does not focus on common human preferences. Also, it does not take event type as a factor, and the albums in the dataset are mostly vacation albums. Moreover, the prediction algorithm is tested on unseen images in the same album used for training, and it does not focus on new album prediction. }

%The work that is most related to our project is work on summarization and selection from an album or several albums\cite{social_summarize, scene_summarize}. For example, using public photo/video collections from the internet for well-known cities such as Rome, the objective is to automatically derive a one page visual summary of a scene or city that captures the key sites of interest\cite{scene_summarize}. This is accomplished by clustering the images based on visual features, such as SIFT.
%{\LARGE Stopped here on the review of previous work - I need to move on to the methods and results!}\cite{nips_summarize} focuses on single album summarization, and they consider coverage and diversity of the albums and do not take image importance score as a cue. \cite{vacation, sum_pinaki} study the problem of summarization of personal albums, using  image quality as well as coverage and diversity as cues. Single image quality is only one of the factors, and image quality is general aesthetics or interestingness which does not take event-specific information into account. \cite{sum_pinaki} aims at summarization of personal photo collections taken in a long time span, and it takes event type as one photo descriptor to calculate diversity and coverage of the photo subset. Instead, we focus on an album taken from a single event, and we use event type information for image importance score prediction.
%\cite{perosnalized_che} proposed a ranking system for photographs based on a set of aesthetic rules and personal preference. Their work is for photography, not personal albums. \cite{gaze} uses gaze information from user's photo viewing process to assist automatic photo selection algorithm, and their work needs additional gaze information from user which is hard to acquire.

%\cite{photo_selection1} is most relevant to our work. In their work, they aim on selection for important photos from single event album, and different factors are considered: image quality, face presentation, concept features, and collection based features. However, each album used for training and test in this work is collected from single participant and the important subset is picked by the same person: it does not focus on the common human preference. Also, it does not take event type as a factor, and the albums in the dataset are most vacation albums. Moreover, the prediction algorithm is tested on unseen images in the same album used for training, and it does not focus on new album prediction. 

Our work differs from all the above in that we focus on: i) whether humans have common preferences for image importance/preference scores, ii) whether image importance can be predicted for unseen albums with widely varying content, and iii) whether event type information is important for the prediction. To summarize, we are introducing a subjective but predictable image property: event-specific image importance, and we propose a method to predict this property.
\section{The Curation of Flickr Events Dataset}
Are people's ratings for images in albums representing particular events predictable? Our intuition is that in an album of a certain event type, there will be a consistent subset of images that will be preferred by most people. However, there is no available dataset to verify this intuition, and test the degree of people's agreement on this highly subjective task. In this section, we describe the collection of the CUration of Flickr Events Dataset (CUFED), and measure the consistency of human subjects' preferences on this dataset. CUFED provides a ground truth dataset that allows us to measure the predictability of human rated image importance scores, and to develop our prediction model. CUFED will be made available on our project webpage. 
\subsection{Album Collection}



\begin{table*}[!htb]
\begin{center}
\small
\centerline{
\begin{tabular}{|  C{2cm} | C{3cm} | C{4.5cm} | C{3.5cm} | C{2.8cm} | }
\hline
\textbf{Categories}                & \textbf{Important Personal Event} & \textbf{Personal Activity} & \textbf{Personal Trip} & \textbf{Holiday} \\ \hline\hline
\textbf{Event types} \newline \textbf{and \# albums} &  Wedding:198 (98\%) \newline Birthday:180 (91\%)\newline Graduation:178 (88\%)
&Protest:50 (92\%)  \newline Personal Music Activity:25 (92\%) \newline Religious Activity:50 (90\%)\newline  Casual Family Gather:50 (84\%) \newline Group Activity:50 (82\%) \newline Personal Sports:100 (78\%)\newline Business Activity:50 (76\%)\newline Personal Art Activity:54 (70\%)
           &  Architecture/Art:50 (92\%)\newline  Urban Trip:100 (89\%)\newline Cruise Trip:50 (88\%) \newline Nature Trip:50 (86\%)\newline Theme Park:100 (86\%)\newline Zoo:99 (85\%) \newline Museum:50 (84\%)\newline  Beach Trip:50 (82\%)  \newline Show:100 (82\%)\newline  Sports Game:50 (58\%) 
             &   Christmas:100 (87\%)\newline Halloween:99 (86\%) \\ \hline
\end{tabular}
}
\end{center}
\caption{23 Event types, their corresponding number of albums, and percentage of significant albums at level $q=0.05$ using Kendall's $W$ statistics. The event types fall into four categories.}
\label{event_type}
\end{table*}



In order to collect a dataset of albums of different event types, we segmented albums from the Yahoo Flickr Creative Commons 100M Dataset (YFCC100M ) \cite{thomee2015yfcc100m}. The YFCC100M Dataset has 100 million images and videos from Flickr. In this collection, each image has the following metadata: which photo uploader was used; the time the image was taken; and often there are user tags. We took advantage of the metadata to segment dataset into albums: For each photo uploader, events are segmented based on timestamp and tags: images taken within short time interval (3 hours) and with more than 1/3 common tags belong to one event. Using tags to filter the data was inspired by the observation that users tend to give the same tags to an event album instead of individually tagging every single image in it. Using this approach, we segmented 1.8 million albums from the YFCC100M dataset. Here, we randomly selected 20,000 albums to work with. 

To get the event type of those albums, we presented the albums to workers on Amazon Merchanical Turk (AMT) and asked them to classify the albums into 23 event types. Aside from these event types, the workers could choose ``Other events," ``Not an event", ``More than single event" or ``Cannot decide" instead of available event types. We chose our 23 event types so that they cover the most common events in our lives, ranging from weddings to sports games. %, and the event types
All 23 event types are shown in Table~\ref{event_type}. Each album was labeled by 3 workers. Over 82\% of the 20k albums received the same labels from at least 2 of the 3 workers. 
If we exclude the albums that at least one worker thought was ``Not an event", 88\% of albums achieved 2 workers or more agreement.
We assigned the event type to each album that had at least 2 votes %[DO YOU MEAN 3, OR DO YOU MEAN "at least 2"?]
, and excluded albums with 3 different votes. This resulted in 16.5 thousand albums.
%assuming those albums to be not an event album. 

We further randomly selected 50-200 albums from each of the event types (except for \textit{Personal Music Activity}, which has 25 albums), resulting in a dataset of 1883 albums. The number of events of each type is shown in Table~\ref{event_type}. The size of the albums varies between 30 and 100 images. We chose these parameters by hand to emphasize our intuition that some event types will have more consistent ratings, and hence more predictability, than others.  Therefore, in this dataset, we emphasized those events in hope of learning more from them.
\subsection{Data Annotation}

In order to get the ratings for each image in an album, we presented an album together with its event type to AMT workers and let them rate each image in that album as very important, important, neutral, or irrelevant. The four ratings are mapped to scores \{2,1,0,-2\} when creating groundtruth. We intentionally did not give specific criteria for the rating levels, to encourage the workers to rate based on their intuition. In our pilot study, workers on AMT tended to mark a large proportion of the images as very important/important. This is understandable, since most of the albums are of high quality, but it leads to a ceiling effect on the ratings. To control the size of images marked as important, we forced the workers to label 5\%-30\% of the images as very important, and 10\%-50\% as important. The average time for each ranking was 7.7 seconds.  Each album was annotated by 5 distinct workers. 292 workers participated in the tasks. Over 90\% of our data has been annotated by 93 workers. 

The image ratings collected from AMT differed in quality among different AMT workers, and low quality worker submission could harm the ground truth data we were trying to collect. In our AMT task, only workers who passed an event recognition from single image qualification test could proceed to the real task. In addition, we added two ``distraction" images which were clearly not related to the event to screen workers who were not paying attention.  However, it was still not possible to assure the quality of an individual submission because of the subjective nature of the image importance rating task. Therefore, in order to filter ``bad" submissions, we found workers who consistently gave scores far from others and filter their submissions (if for one worker, more than 30\% of his/her submissions had a higher euclidean distance from the average of other workers' submissions, that worker's submissions were filtered out). This was based on the observation that the quality of submissions a worker gave were consistent. Only two workers were filtered out in this way.


%\begin{table*}[t]
%\begin{center}
%\small
%\begin{tabular}{|  C{2cm} | C{3cm} | C{4.5cm} | C{3.5cm} | C{2.8cm} | }
%\hline
%\textbf{Categories}                & \textbf{Important Personal Event} & \textbf{Personal Activity} & \textbf{Personal Trip} & \textbf{Holiday} \\ \hline\hline
%\textbf{Event types} \newline \textbf{and \# albums} &  Wedding:198 (98\%) \newline Birthday:180 (89\%)\newline Graduation:178 (87\%)

%&Protest:50 (94\%)  \newline Religious Activity:50 (92\%)\newline Personal Music Activity:25 (88\%) \newline Casual Family Gather:50 (86\%) \newline Group Activity:50 (82\%) \newline Personal Sports:100 (81\%)\newline Business Activity:50 (76\%)\newline Personal Art Activity:54 (70\%)
%           &   Cruise Trip:50 (88\%) \newline Nature Trip:50 (88\%)\newline Urban Trip:100 (87\%)\newline Zoo:99 (87\%)\newline Architecture/Art:50 (86\%)\newline Theme Park:100 (85\%)\newline Beach Trip:50 (84\%)\newline Museum:50 (80\%)\newline Show:100 (78\%)\newline  Sports Game:50 (62\%) 
%             &   Christmas:100 (86\%)\newline Halloween:99 (82\%) \\ \hline
%\end{tabular}
%\end{center}
%\caption{23 Event types, their corresponding number of albums, and percentage of significant albums at level $q=0.05$ using Kendall's $W$ statistics. The event types fall into four categories.}
%\label{event_type}
%\end{table*}


\subsection{Consistency Analysis}
\label{consistency_section}

To examine the consistency of the human rating of images, we split our subjects into two independent halves in each album, and used Spearman's rank correlation ($\rho$) to evaluate consistency. For each album, we averaged the correlation scores of all possible random splits. The average correlation over all albums was 0.40. 

We further evaluated the annotation consistency with Kendall's $W$, which directly calculates the agreement among multiple raters, and accounts for tied ranks. Kendall's $W$ ranges from 0 (no agreement) to 1 (complete agreement). Note that in our workers' rating of one album, tied ranks are very frequent, since there are only 4 possible ratings, and the average album size is 52. Average Kendall's $W$ over all albums was 0.40. Both Spearman's rank correlation $\rho$ and Kendall's $W$ showed significant consistency across subjects despite the high subjectiveness of this problem.

To test the statistical significance of Kendall's $W$ score, we did a permutation test over $W$ to obtain the distribution of $W$ under the null hypothesis, and for each event type, we used the Benjamini-Hochberg procedure to control the false discovery rate (FDR) for multiple comparisons \cite{Benjamini01thecontrol}. At level $q = 0.05$, 86\% of albums were significant on average. The different percentages of significant albums in different event types confirmed our intuition that different events receive different human consistency. Wedding received highest consistency, with 98\% of albums as significant; while sports game had 58\% significant albums, which was the lowest among the 23 events. This also confirmed our intuition. In the supplementary material \cite{supplementary}, we include examples of albums that received high consistency and low consistency.

\section{Approach}
In this section, we propose a Convolutional Neural Network (CNN) based method for estimating an event-specific image importance score in an album given the event type of this album.

\subsection{CNN Structure}
\label{CNN_section}
The design of our CNN architecture is shown in Fig.~\ref{figure1}. It has several properties:

\begin{figure*}[!htb]
\begin{center}
\includegraphics[width=6in]{architecture3}
\end{center}
\caption{A siamese CNN architecture for joint training over events. A pair of images from the same album is the input to the two identical network branches that share the same set of parameters. Intermediate layers are omitted here for simplicity. In the last fully connected layer, only the units corresponding to the correct event type are activated and back-propagated. The output score in the last fully connected layer from one half of the network is the prediction of the importance score of the corresponding input image. The piecewise Ranking loss is used to train the network.}
\label{figure1}
\end{figure*}

\subsubsection{Feature sharing among event types}  
\label{section_1}
We train a single network with albums from all event types as training data. All event types share the man CNN branch. The reasons are as follows. First, there exists strong visual correlation among different event types in terms of image importance, therefore for specific event type, labeled data from other event types will help as "implicit" data augmentation. Second, feature sharing will significantly reduce the number of parameters in the network and regularize the network training, and especially for our problem, high variance among albums within each event type and relatively small dataset make this even more desirable. Therefore, in our network, all event types share the features except for the branched out last layer, and during the training/test process, only the branch corresponding to the event type of image pair is activated and back-propagated.

\subsubsection{2-stage progressive training} 
Due to the large variation among albums and the relatively small scale of dataset (especially for some event types such as \textit{casual family/friends gathering}), directly training CNN for separate event types as in Section~\ref{section_1} may lead to over-fitting for some event types with less training data. Therefore, we introduce the 2-stage progressive learning: we cluster the 23 event types into a set of superclasses based on the similarity between event types. In the first stage, we train the network with superclass information, and in the second stage, we progressively advance to train the network with the subordinate class (23 event types) information. Initialization of the second stage is done using the network from the first stage. This helps in that i) the last layer is trained on more data in the first stage, and ii) the relationship between event types is taken into account: some event types are more similar to each other, sharing similar concepts, important moments or important visual features. Similar event types should share similar image preferences. Our two-stage learning system leverages that intuition. %By utilizing this, we set the initialization of 2-stage in a reasonable place.

 \subsubsection{Siamese architecture} 
 The  nature of the human image rating task for an album leads to the absolute image score not being reliable, and comparison is only valid within one album. Each album has different quality, while in our AMT task, a participant only sees one album at a time, and the participant's judgement for each image might be biased by the quality of the album; moreover, there is no reliable way to find out the quality of one album. Therefore, rather than an absolute image score, we use the score difference between a pair of images to train the network. The network architecture is a siamese network architecture \cite{siamese}. In the siamese network, there are two identical parts that share the same architecture and weights. Each part takes one of the image pair as input, as shown in Fig.~\ref{figure1}.  Biased image scores also means that images from different albums cannot be compared directly, so the image pair being compared should come from the same album.
 
\subsubsection{Piecewise ranking loss} 
For each input image pair to the network $(\text{I}_{1}, \text{I}_{2})$, $G(\text{I})$ is the ground truth score of  image $I$, and $P(\text{I})$ is its predicted score from the network. We use a piecewise ranking loss (PR loss) to train our siamese network:

\begin{equation} 
  \text{PR} = \begin{cases}
 \frac{1}{2}\text{max}(0, \left | D_\text{p} \right |-\text{m}_\text{s})^{2} &\text{if}  \; D_\text{g} < \text{m}_\text{s}\\
\mathrlap{\frac{1}{2}\left \{ \text{max}(0, \text{m}_\text{s}-D_\text{p})^{2} + \text{max}(0, D_\text{p}-\text{m}_\text{d})^{2} \right \}} \\
 & \text{if} \;  \text{m}_\text{s} \leq D_\text{g}  < \text{m}_\text{d} \\
 \frac{1}{2}\text{max}(0, \text{m}_\text{d}-D_\text{p})^{2} \; \; \; \; \; \;  \; \;\; \;  \; \; \; \;   & \text{if}  \; D_\text{g}  > \text{m}_\text{d}
\end{cases}
\label{loss_function}
\end{equation}

where  $D_\text{g} = G(\text{I}_{1})-G(\text{I}_{2})$ is the ground truth score difference between the input image pair, and  $D_\text{p} = P(\text{I}_{1})-P(\text{I}_{2})$ is the predicted score difference. $\text{m}_\text{s}$ and $\text{m}_\text{d}$ are predefined values for similar and different margins. In Equation ~\ref{loss_function}, several conditions are considered:

\begin{itemize}
  \item When $D_\text{g}  > \text{m}_\text{d}$, the loss function reduces to a variation of ranking SVM hinge loss \cite{ranking_loss}. We use L-2 loss which penalizes high errors more heavily than traditional hinge loss \cite{svm}. This is similar to contrastive loss function when the input pair of images are deemed dissimilar \cite{contrastive}, but we are not using the euclidean distance of the output of the network, since the sign of $D_\text{p}$ is important here. 
  \item When $D_\text{g} < \text{m}_\text{s}$, the loss function reduces to a variation of contrastive loss when the input pair is deemed similar \cite{contrastive}. In addition to the contrastive loss in \cite{contrastive}, we introduce a margin: $ \text{m}_\text{s}$. The margin serves as a slack term. The reason to have it is that the ground truth importance score is acquired from a group of humans, and the variance is relatively high among the humans, as shown in Section~\ref{consistency_section}. The introduction of relaxation with $\text{m}_\text{s}$ makes the network less sensitive to this variance in our ground truth.
  \item When $\text{m}_\text{s} < D_\text{g}  < \text{m}_\text{d}$, the loss function will only penalize the $D_\text{p}$ not being in the same range with $D_\text{g}$. This pulls $D_\text{p}$ towards $D_\text{g}$ when the image pair is ``somewhat" similar to each other, meanwhile reducing the loss function's vulnerability to the variance in our ground truth.
\end{itemize}
The PR objective loss function has the following advantages: It is trained from image pairs with any score difference in one album, thus making a full use of the training dataset; It introduces relaxation to the ground truth score, thus making the network more stable, which is beneficial in our subjective problem.


\subsection{Face Heatmap Incorporation}
Faces are an important factor in an image, and images with faces tend to be more attractive and more interesting \cite{sum_pinaki}. Moreover, our intuition is that: in an event album, images containing important people in this event are more important. This feature across images cannot be captured by the CNN trained with image pairs. In order to incorporate face information, we generate face heatmaps to separately train a shallow CNN to predict the importance score of original photos. A separate face heatmap-based score enables more flexible tuning of the relative strength of the two scores from original images and face heatmaps. Face heatmap CNN training follows the same pipeline as Section~\ref{CNN_section}.

Our face heatmaps are generated following the state-of-the-art face detection \cite{Li_2015_CVPR}. In order to get face identity representation, we train 18 CNN models for different face parts and concatenate resulting FC layers as final face descriptor following similar pipeline as \cite{Sun_2014_CVPR}, we then do agglomerative identity clustering to get frequent faces in an album. Faces with higher frequency are viewed as important identities. In the face heatmap, faces are represented  with Gaussian kernels, and important people are emphasized with higher peak values.
% Moreover, peak value of Gaussian kernels is modulated by the quality score of that face. This is inspired by the observation that low quality faces are not as appealing as high quality faces.
Examples of face heatmaps are shown in Fig.~\ref{figure4}. In the testing stage, the prediction from the original image and the face heatmap are combined according to the following formula:
\begin{equation}
P=P_{\text{I}} + {\lambda} \cdot \text{min}\left \{ \text{max}\left \{ P_{\text{f} }, \beta \right \},\alpha \right \}
\label{equa-face}
\end{equation}
where $(\text{P}_{\text{I}}, \text{P}_{\text{f}})$ are predicted scores from the photo and face heatmap respectively. The face heatmap contains a limited information, therefore we constrain the effect of the face heatmap for the final prediction with $(\alpha, \beta)$, so that extreme predictions from the face heatmap are eliminated; $\lambda$ is also used to further control the effect of the face heatmap-based prediction.

\begin{figure}[h]
\centering
\includegraphics[width=3.3in]{section3_1}
\caption{Face heatmap generation example. Images are from one wedding event album. Four images in the first row are the original images, and the images in the second row are the corresponding face heatmaps. Faces of important people have higher peak values. Images in the second column show that face detection is not ideal, and images in the third column show that identity clustering is not perfect either.}
\label{figure4}
\end{figure}
 
\section{Experimental Results}
In this section, we compare our result with several baseline methods.
\subsection{Experimental Settings}
\paragraph{Dataset}
For training and testing, we randomly split the  Curation of Flickr Events Dataset into 3:1 albums for every event type. Therefore, the training set consists of 1404 albums, and the test set has 479 albums.  For hyperparameters in face information incorporation, we use a grid search to decide their values by 5-fold validation. 

\paragraph{Parameter setting}
Our CNN architecture for original image input is the same as \cite{imagenet}. Since we have a limited dataset, we fine-tune the network from a pre-trained model \cite{caffe}. In Fig.~\ref{figure1}, the penultimate fully connected layer (the layer before the branched-out event-specific score prediction layer) corresponds to the 7-th fully connected layer in Alexnet \cite{caffe}, and the branched-out event-specific score prediction layer is followed by a sigmoid layer. For PR loss, we set $\text{m}_\text{s} = 0.1$ and $\text{m}_\text{d} = 0.3$. For training parameters, we use the default settings for pre-training in Caffe \cite{caffe}, but we start from a smaller learning rate of 0.001 \cite{rcnn}. 

We follow \cite{imagenet} 's data augmentation approach: Input images are resized to $256\times 256$. During the training stage, images are randomly cropped to $227 \times 227$ crops, and there is a 50\% probability that input images are horizontally flipped; In the test stage, predictions are averaged on four crops (four corners and the center) and their horizontal reflections.

We train 5 different CNNs in our 5-fold cross validation,  and use an ensemble of the networks for the final prediction. The cross-validation ensemble can improve the generalization of the network by increasing the ambiguity \cite{ensemble}. In the first stage of our progressive learning, we use only one event cluster, which means all event types are categorized into one superclass. 

For the face heatmap, we train a CNN with one convolutional layer and two fully connected layers from scratch. The network architecture is shown in the supplementary material \cite{supplementary}. 

\paragraph{Evaluation metrics} 
We use two evaluation methods to compare the different approaches. For both evaluation methods, we assume that given an event album, we view the top $t\%$ images as relevant images. 

First, we use mean average precision(MAP) to evaluate our models. MAP is a common evaluation method for information retrieval. It is the averaged area under the precision-recall curve. Given an event album, and top $t\%$ of the images as being relevant images, $\text{MAP}@(t\%)$ can be calculated:
\begin{equation}
\textup{AP(S)}@t\%=\int_{0}^{1}p(r)d(r)\approx \frac{\sum_{k=1}^{n}p(k)\times \textup{rel}(k)}{\left \lceil n\cdot t\% \right \rceil}
\end{equation}
\begin{equation}
\textup{MAP(U)}@t\% = \frac{1}{N}\sum_{i=1}^{N}\textup{AP}(\textup{S}_{i})@t\%
\end{equation}
where $\text{S}_{i}$ is the $i$th album, and $\text{U}$ is the collection of all albums. $n$ is the size of album $S$,  $p(k)$ is the precision at rank $k$, and $\text{rel}$ is an indicator of whether the $k$th ranked image from our algorithm is an relevant image, i.e. among the top t\% ground truth.

Second, we calculate the precision($P$) for evaluation. For one album, if we retrieve the top $t\%$ of the images, we can calculate the precision $P$, computed as the ratio between the number of relevant photos in retrieved images and the size of all relevant images. Different from MAP, $P$ cares entirely about how many important images can be retrieved at a cut-off level, and do not care about the position they are in the retrieval list, or where the rest of important images are in the ranking system. Although less informative than MAP, $P$ is also an intuitive way to demonstrate the effectiveness of our predicted image ranking result.

For a set of $t\%$ values, we may compute a set of corresponding $\text{MAP}$s and $P$s, then we can draw a  $\text{MAP} @ t\%$  as well as a $P@t\%$ curve. Since we are solving an image selection problem, we care more about MAP and $P$ for small $t\%$, so we only present curves for $t \leq 30$.


\subsection{Results and Analysis}
In this section, we compare our method Piecewise Ranking - CNN trained progressively on all event types (PR-CNN(Progressive)) with different baselines, and show from different perspectives the advantages of our method. The comparison of all the methods for each of the 23 event types are shown in supplementary material \cite{supplementary}. 
\subsubsection{Does aesthetics play an important role?}
In a user study, Walber \etal show that that humans uses visual appeal of an image as a criterion for selecting important images in an album \cite{gaze}. Here, in order to quantify the role attractiveness plays in the selection, we use an aesthetic score prediction to predict the image importance score. We use a warped image as input to train a CNN classifier similar to \cite{aesthe_14}. Table~\ref{aesthetic_table} shows that the aesthetic score of images is only slightly better than random. Therefore, we conclude that aesthetics is not a very important criterion for human selection for important images in event albums. However, in the supplementary material \cite{supplementary}, we observe that the aesthetic score is more predictive for some events than others, e.g. \textit{Nature trip}, \textit{Personal art activity} (in which many photos are portrait shots). This is consistent with our intuition: aesthetics is an important criterion for human selection in events without strong narrative structure.


\begin{table*}[]
\begin{center}
\small
\begin{tabular}{c|cccccc|cccccc}
\hline
          & \multicolumn{6}{c|}{$\text{MAP}@t\%$}          & \multicolumn{6}{c}{$P@t\%$} \\ \hline  \hline
t\%       & 5     & 10    & 15    & 20    & 25    & 30    & 5  & 10  & 15 & 20 & 25 & 30 \\ \hline
Random & 0.122 & 0.164 & 0.211 & 0.260 & 0.305 & 0.350 & 0.058  &  0.093   &  0.141  &  0.195  & 0.251   & 0.298   \\
Worker & 0.328 & 0.410 & 0.476 & 0.531 & 0.580 & 0.624 & 0.242   &  0.371   &  0.448  &  0.505  & 0.552   & 0.591   \\ \hline
Aesthetic & 0.139 & 0.191 & 0.242 & 0.290 & 0.338 & 0.384 & 0.060   &  0.121    &   0.176 &    0.228&0.284    &    0.335\\ 
Pre-KNN & 0.220 & 0.276 & 0.326 & 0.373 & 0.419 & 0.465 &  0.138   &  0.216   & 0.275   & 0.326   &   0.372 & 0.419    \\
Pre-SVM &0.252&0.320&0.370&0.420&0.466&0.512&0.169&0.262&0.318&0.363&0.410&0.458 \\
SVM-CNN & 0.268 & 0.336 & 0.391 & 0.447 & 0.496 & 0.543 &  0.173  &  0.269   &  0.337  &   0.388 &   0.439 &    0.485\\ \hline
NoEvent-CNN & 0.261 & 0.318 & 0.369 & 0.422 & 0.474 & 0.520 &  0.167  &  0.247   &    0.310&    0.372&  0.425  & 0.468\\
PR-CNN(Direct)    & 0.296 & 0.358 & 0.410 & 0.462 & 0.511 &  0.557     &  0.199  &  0.293   &    0.352&    0.403&  0.454  &   0.498 \\ 
PR-CNN(Progressive)    &0.302&0.361&0.415&0.469&0.517&0.563&0.214&0.296&0.356&0.410&0.458&0.502 \\
Ensemble-CNN & \textbf{0.305}& \textbf{0.364}& \textbf{0.417}& \textbf{0.471}& \textbf{0.519}& \textbf{0.563}& \textbf{0.216}& 
\textbf{0.301}& \textbf{0.360}& \textbf{0.411}& \textbf{0.459}& \textbf{0.504} \\ \hline 

%Ensemble-CNN + face &0.307&0.365&0.419&0.472&0.519&0.563&0.214&0.305&0.360&0.412&0.460&0.502 \\ \hline
%Ensemble-CNN (\textit{part}) & 0.280& 0.340& 0.397& 0.453& 0.499& 0.543& 0.206& 0.285& 0.339& 0.391& 0.440& 0.490 \\ 
%Ensemble-CNN + face (\textit{part})&0.283&0.341&0.399&0.455&0.501&0.544&0.206&0.287&0.340&0.394&0.440&0.489 \\ \hline

\end{tabular}
\end{center}
\caption{Comparison of predictions using different methods. The evaluation metric here is $\text{MAP}@t\%$ and $P@t\%$. The random ranking score is also shown as a lower bound.}

\label{aesthetic_table}
\end{table*}


%\begin{table*}[]
%\centering
%\small
%\caption{Comparison of predictions using different methods. Evaluation metric here is $\text{MAP}@t\%$ and $P@t\%$. Random ranking score is also shown as a lower bound.}
%\begin{tabular}{c|cccccc|cccccc}
%\hline
 %         & \multicolumn{6}{c|}{$\text{MAP}@t\%$}          & \multicolumn{6}{c}{$P@t\%$} \\ \hline  \hline
%t\%       & 5     & 10    & 15    & 20    & 25    & 30    & 5  & 10  & 15 & 20 & 25 & 30 \\ \hline
%Random & 0.120 & 0.162 & 0.211 & 0.259 & 0.304 & 0.349 & 0.051  &  0.091   &  0.141  &  0.194  & 0.250   & 0.3298   \\
%Worker & 0.328 & 0.412 & 0.477 & 0.533 & 0.583 & 0.627 & 0.251   &  0.381   &  0.457  &  0.514  & 0.560   & 0.598   \\ \hline
%Aesthetic & 0.141 & 0.193 & 0.243 & 0.291 & 0.338 & 0.385 & 0.062   &  0.124    &   0.178 &    0.228&0.283    &    0.334\\ 
%Pre-KNN & 0.219 & 0.278 & 0.326 & 0.372 & 0.420 & 0.465 &  0.134   &  0.214   & 0.272   & 0.320   &   0.370 & 0.414    \\
%Pre-SVM &&&&&&&&&&&& \\ \hline
%SVM-CNN & 0.251 & 0.323 & 0.381 & 0.433 & 0.483 & 0.528 &  0.166  &  0.263   &  0.324  &   0.380 &   0.425 &    0.476\\
%NoEvent-CNN & 0.259 & 0.318 & 0.369 & 0.422 & 0.472 & 0.518 &  0.167  &  0.247   &    0.309&    0.372&  0.423  & 0.466\\
%PR-CNN(Direct)    & 0.291 & 0.355 & 0.406 & 0.455 & 0.502 &  0.547     &  0.200  &  0.287   &    0.345&    0.398&  0.445  &   0.486 \\ 
%PR-CNN(Progressive)    &0.299&0.362&0.413&0.466&0.514&0.559&0.211&0.298&0.354&0.408&0.456&0.502 \\ \hline
%Ensemble-CNN &&&&&&&&&&&& \\
%Ensemble-CNN + face &&&&&&&&&&&& \\ \hline

%\end{tabular}
%\label{aesthetic_table}
%\end{table*}


\subsubsection{Are pre-trained CNN features useful?}
Pre-trained CNN features have been shown to have a high generalization ability to new tasks \cite{devil, rcnn}. We use pre-trained CNN model \cite{caffe} with the architecture from \cite{imagenet} as our feature extractor. The seventh layer before the last output layer has 4096 units, and we use the 4096-d vector as the visual feature for each input image. For each test image, we perform a K-nearest neighbors (KNN) search against all training images in the same event type, and use the top 10 retrieved images to predict the test image's score. Prediction is the weighted average of the 10 images' ground truth importance score, where the weight is the image's similarity score to the query test image. We denote this method as Pre-KNN.

In addition to simple KNN, we also use the 4096-d feature vector to train a Ranking SVM for each event type. This method is denoted as Pre-SVM.

Table~\ref{aesthetic_table} shows the result of using pre-trained CNN feature. The KNN method significantly outperforms using the aesthetic score and random ranking. However, it is still much lower than our proposed method. This shows that high variation of albums makes the direct score prediction using images in other albums with similar visual appearance unreliable. The Pre-SVM method performs better than the KNN method, but the improvement is limited. 

The results of the above two experiments verify that the pre-trained CNN features can generalize to the event-based image importance prediction problem.

\subsubsection{Is Piecewise Ranking loss necessary?}
In order to show the advantage of PR loss, we compare our result with the result trained from a conventional ranking SVM hinge loss. For the SVM ranking loss, the network architecture is exactly the same as our proposed method except that the loss function is the SVM ranking loss:
\begin{equation} 
\text{L}(\text{I}_{1}, \text{I}_{2}) = \text{max}(0, 1-D_\text{p}) 
\end{equation}
where $D_\text{p} = P(\text{I}_{1})-P(\text{I}_{2})$ is the predicted score difference between the image pair.

This method is denoted as SVM-CNN. As shown in Table~\ref{aesthetic_table}, PR-CNN(direct) outperforms Ranking SVM hinge loss especially when $t < 20$. Ranking SVM uses 87\% of image pairs as the training data compared to PR loss (it does not use the image pairs with the same score in an album). The reason for PR's better performance may be i) introducing the loss under similar/somewhat similar condition, which is more informative than the hinge loss, and ii) having 15\% more training data.

We also used a single column network and Euclidean Loss to directly predict the importance of a single image, and the results were consistently worse than SVM-CNN by 0.5 -1\%. Due to limited space, we skip listing it in the main paper and only show it in the supplementary material \cite{supplementary}.

\subsubsection{Is event information useful?}
In the previous work on album summarization or photo selection, a common approach is to use general image interestingness/quality to represent the image importance score irrespective of the event type of the album \cite{photo_selection1, vacation, sum_pinaki} . We propose that event type information is an important factor in determining the image importance score,  and that progressively using 2-stage learning will help with the prediction. In this section, we verify our proposal by comparing the performance of CNNs trained i) without the event type information, ii) with 2-stage learning, and iii) with only the second stage learning on 23 event types. 

We train a CNN with exactly the same architecture and training parameters except that the last layer of each of the halves of the siamese network in Fig~\ref{figure1} is one unit, and there is no input event type information. Image pairs from any event type are viewed as from the same ``superclass". This method is denoted as No Event CNN(NoEvent-CNN).  As shown in Table~\ref{aesthetic_table}, although trained with PR loss, without event type information, the network performs worse than PR-CNN(Progressive) by a large margin of about 4.5\% steadily on MAP score. In addition, the difference of $P@t\%$ is especially large for smaller $t$, which we care more about.

We also train a CNN with only the second stage directly on 23 event types, as PR-CNN(Direct). Table~\ref{aesthetic_table} shows the performance gain using 2-stage learning is about 1\%, and is especially large for smaller $t\%$.
  
\subsubsection{Incorporation of face information}
In order to incorporate the face information, we use the 5-fold cross validation inside the training set. The ensemble of the 5-fold validation PR-CNN(Progressive) is denoted as Ensemble-CNN. Table~\ref{aesthetic_table} shows that the ensemble of cross-validation networks can substantially improve the performance. 

The parameters \{$\alpha, \beta, \lambda$\} in Equation~\ref{equa-face} are learned with grid search on the 5-fold cross validation. Among 23 event types, only 10 event types show a performance gain after face information is incorporated in the validation set, and thus the face information is used for only these 10 event types on test set. Table~\ref{face_table} shows the effect of face information for some example event types. As shown, for some event types, face substantially helps with the performance, while for some other event types, face information has little impact, or even harms the performance. Incorporating face information helps the overall performance. We present the result for all 10 event types in the supplementary material \cite{supplementary}. 

\begin{table}[]
\begin{center}
\small
\centerline{
\begin{tabular}{c|ccc}
\hline
%\multicolumn{1}{l|}{} & \multicolumn{3}{c}{$\text{MAP}@t\%$}          \\ \hline
t\%                   & 5             & 15            & 25            \\ \hline
Beach Trip            & 0.353(+0.051) & 0.455(+0.022) & 0.555(+0.011) \\
Nature Trip            & 0.167(+0.008) & 0.272(+0.008) & 0.369(+0.07) \\
Group Activity            & 0.315(+0.003) & 0.489(+0.001) & 0.586(+0.03) \\
Halloween            & 0.315(+0.000) & 0.424(+0.001) & 0.529(+0.02) \\ 
Museum            & 0.293(-0.010) & 0.368(-0.010) & 0.453(-0.06) \\\hline
\end{tabular}
}
\end{center}
\caption{For a given event type, $\text{MAP}@t\%$ for Ensemble-CNN after incorporation of face information. Difference between after v.s. before face information is also shown. Only 5 example event types are shown here. }
\label{face_table}
\end{table}



%Therefore, we give a comparison for only those 14 event types: Ensemble-CNN (\textit{part}) and Ensemble-CNN + face (\textit{part}). As shown, the improvement of MAP score by face information incorporation is steady.

\section{Conclusion}
In this work, we introduce a new image property: event-specific image importance. We provide a new dataset consisting of common personal life events, and we provide human generated image importance score ground truth for the dataset. We provide evidence that although the event-specific image importance score is  subjective, it is a well-defined and predictable property: there is consistency among different subjects. We develop a CNN-based system to predict event-specific image importance. We show that although aesthetics is usually considered in an image selection system, it is not the most important criterion for people. More importantly, we also show that the event information is an important criterion when people select important images in an album. In our prediction system, we design a Piecewise Ranking Loss for a dataset with subjective or high variance ground truth, and we use a 2-stage progressive learning process to train the network. We show that our system is advantageous over the conventional Ranking SVM loss and training procedure. 

This work is the first attempt to predict event-specific image importance. This image property is especially useful in album summarization and image selection from an album. In future work, it will be interesting to further investigate the relationship between event types, and to deal with albums with multiple/ambiguous event types.  Also, it is interesting to develop a combinatorial curation system based on the image importance score, taking diversity and coverage into consideration. Our  Curation of Flickr Events Dataset will be public on our project webpage.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

